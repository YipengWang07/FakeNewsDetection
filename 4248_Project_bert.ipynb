{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMjPBGi8WQc_"},"outputs":[],"source":["import pandas as pd\n","import re\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdjg8FWOUhgr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647322917091,"user_tz":-480,"elapsed":2450,"user":{"displayName":"SH Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01104111170945211457"}},"outputId":"042d4d6f-952c-4f64-f294-1ff1d2c63569"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CujNrVsaV1GK","outputId":"41383a33-f35c-45ec-90a7-a56fdafd8652","executionInfo":{"status":"ok","timestamp":1647322917091,"user_tz":-480,"elapsed":4,"user":{"displayName":"SH Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01104111170945211457"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/raw_data\n"]}],"source":["%cd /content/drive/My Drive/Colab Notebooks/raw_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPSmbFtT2egC"},"outputs":[],"source":["train = pd.read_csv(\"fulltrain.csv\",header=None)\n","text = train.iloc[:,1]\n","test = pd.read_csv(\"balancedtest.csv\",header=None)\n","t_text = test.iloc[:,1]"]},{"cell_type":"markdown","source":["### Bert init steps\n"],"metadata":{"id":"TsMoYlUmKqCm"}},{"cell_type":"code","source":["# A dependency of the preprocessing for BERT inputs\n","!pip install -q -U \"tensorflow-text==2.8.*\"\n","!pip install -q tf-models-official==2.7.0"],"metadata":{"id":"QxaQRkaSEdKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as tf_text\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","import matplotlib.pyplot as plt\n","\n","tf.get_logger().setLevel('ERROR')"],"metadata":{"id":"hNyMB0NTEiZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'electra_base':\n","        'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed':\n","        'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n","    'electra_small':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","bert_model = hub.KerasLayer(tfhub_handle_encoder)"],"metadata":{"id":"PwqMiyjWEr8Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RHg0YZFZPGD"},"source":["## Preprocessing for Text"]},{"cell_type":"code","source":["text=train['Text']\n","t_text=test['Text']"],"metadata":{"id":"iqFjZd7XR47B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFTwy0bKZSlm"},"outputs":[],"source":["#Delete HTML\n","from bs4 import BeautifulSoup\n","text = text.apply(lambda x: BeautifulSoup(x,'html.parser').get_text())\n","t_text = t_text.apply(lambda x: BeautifulSoup(x,'html.parser').get_text())"]},{"cell_type":"code","source":["#Remove emoji\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","text=text.apply(lambda x: remove_emoji(x))\n","t_text=t_text.apply(lambda x: remove_emoji(x))"],"metadata":{"id":"Je-tySnk9Ytl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnuJkkTQaPYS"},"outputs":[],"source":["#Transform to lower letter\n","text = text.apply(lambda x: x.lower())\n","t_text = t_text.apply(lambda x: x.lower())"]},{"cell_type":"code","source":["# segment articles to shorter sentences\n","import nltk\n","nltk.download('punkt')\n","text = text.apply(lambda x: nltk.tokenize.sent_tokenize(x))\n","t_text = t_text.apply(lambda x: nltk.tokenize.sent_tokenize(x))\n","text = text.apply(lambda x: [\" \".join(x[i:i+3]) for i in range(0,len(x),3)])\n","t_text = t_text.apply(lambda x: [\" \".join(x[i:i+3]) for i in range(0,len(x),3)])\n","train['Text'] = text\n","test['Text'] = t_text\n","train = train.explode('Text')\n","test = test.explode('Text')"],"metadata":{"id":"G2PsQGJ7L50G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMYg-dlQrRGy"},"outputs":[],"source":["text=train['Text']\n","t_text=test['Text']\n","#Remove punctuation\n","import re\n","text = text.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n","t_text = t_text.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jNN93tCrfyL"},"outputs":[],"source":["#Substitute number\n","import inflect\n","def to_digit(digit):\n","    i = inflect.engine()\n","    if digit.isdigit():\n","        output = i.number_to_words(digit)\n","    else:\n","        output = digit\n","    return output\n","text = text.apply(lambda x: to_digit(x))\n","t_text = t_text.apply(lambda x: to_digit(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sidZqXeGr2sW"},"outputs":[],"source":["#Remove the stopwords\n","# import nltk\n","# nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop=stopwords.words('english')\n","text=text.apply(lambda sen:\" \".join(x for x in sen.split() if x not in stop))\n","t_text=t_text.apply(lambda sen:\" \".join(x for x in sen.split() if x not in stop))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBKk-On-sCs4"},"outputs":[],"source":["#Remove the frequency words\n","freq=pd.Series(' '.join(text).split()).value_counts()[:10]\n","freq=list(freq.index)\n","text=text.apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))\n","t_text=t_text.apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72TJWeU2sVO6","outputId":"a8cef506-0a32-44c0-eb22-ced03e82df66","executionInfo":{"status":"ok","timestamp":1647276745469,"user_tz":-480,"elapsed":9773,"user":{"displayName":"SH Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01104111170945211457"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    many actors seem content churn performances qu...\n","0    ive ive done bridges rattling laundry list fil...\n","0    waste anything else powerful though made clear...\n","0    towels bathrobes carefully placed plants added...\n","0    part jeffs fans nothing supportive wow whether...\n","Name: Text, dtype: object"]},"metadata":{},"execution_count":104}],"source":["# Remove the scarce word\n","freq = pd.Series(' '.join(text).split()).value_counts()[-10:]\n","freq = list(freq.index)\n","text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n","t_text = t_text.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n","t_text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qs3qnGw_g4jI"},"outputs":[],"source":["#Noise Removal\n","def text_cleaner(text):\n","    rules = [\n","        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n","        {r'\\s+': u' '},  # replace consecutive spaces\n","        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n","        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n","        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n","        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n","        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n","        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n","        {r'^\\s+': u''}  # remove spaces at the beginning\n","    ]\n","    for rule in rules:\n","      for (k, v) in rule.items():\n","        regex = re.compile(k)\n","        text = regex.sub(v, text)\n","      text = text.rstrip()\n","    return text.lower()\n","\n","text=text.apply(lambda x: text_cleaner(x))\n","t_text=t_text.apply(lambda x: text_cleaner(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PAFr-C0ykyf"},"outputs":[],"source":["#Stemming\n","from nltk.stem.porter import PorterStemmer\n","ps = PorterStemmer()\n","text=text.apply(lambda x:\" \".join(ps.stem(word) for word in x.split()))\n","t_text=t_text.apply(lambda x:\" \".join(ps.stem(word) for word in x.split()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BABrFujBydWX","outputId":"edf62fb2-8864-4e49-ff27-3e3d9ea092c6","executionInfo":{"status":"ok","timestamp":1647277280398,"user_tz":-480,"elapsed":99256,"user":{"displayName":"SH Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01104111170945211457"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["#Lemmatization\n","from textblob import Word\n","import nltk\n","nltk.download('wordnet')\n","text=text.apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n","t_text=t_text.apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))"]},{"cell_type":"code","source":["## better for next time run\n","train['Text'] = text\n","test['Text'] = t_text\n","train.to_csv('train_pre.csv', index=False)\n","test.to_csv('test_pre.csv', index=False)"],"metadata":{"id":"swVAjMv5Wbx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Model fitting"],"metadata":{"id":"Xcfd5rAofgMf"}},{"cell_type":"markdown","source":["### previous self-trained bert encoder"],"metadata":{"id":"I9ev5FI877TN"}},{"cell_type":"code","source":["import tensorflow_hub as hub\n","import tensorflow as tf\n","from tensorflow.keras.models import Model \n","import bert\n","max_seq_length = 512 \n","input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n","input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n","segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n","# pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","# model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"],"metadata":{"id":"2ptMmSW1NJOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from bert.tokenization import FullTokenizer\n","tokenizer = FullTokenizer(vocab_file, do_lower_case)\n","def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","            \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        pad_len = max_len - len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"],"metadata":{"id":"jOuwpJhsVK09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens,masks,segments = bert_encode(text, tokenizer)"],"metadata":{"id":"lc4hF6efPJUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_tokens,_,_ = bert_encode(t_text, tokenizer)"],"metadata":{"id":"BiVRVNjPgvqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Final pre-trained bert encoder"],"metadata":{"id":"SVbCT9Kb8EcZ"}},{"cell_type":"code","source":["train_pre = pd.read_csv(\"train_pre.csv\",header=0)\n","train_pre.dropna(inplace=True)\n","train_pre.reset_index(inplace=True,drop=True)\n","# load sections of text to avoid ram crash\n","text = train_pre.iloc[415000:,1]\n","\n","test_pre = pd.read_csv(\"test_pre.csv\",header=0)\n","test_pre.dropna(inplace=True)\n","test_pre.reset_index(inplace=True,drop=True)\n","t_text = test_pre['Text']\n","\n","#print(\"Train file start encode\")\n","#text_prepro = text.apply(lambda x: bert_preprocess_model([x]))\n","#text_encode = text_prepro.apply(lambda x: bert_model(x)[\"pooled_output\"].numpy())\n","## initial saving\n","#text_encode.to_csv('train_encode.csv',index=False)\n","## continue saving\n","#text_encode.to_csv('train_encode.csv', mode='a',index=False, header=False)\n","#print(\"Complete\")\n","\n","#print(\"Test file start encode\")\n","#t_text_prepro = t_text.apply(lambda x: bert_preprocess_model([x]))\n","#t_text_prepro =  bert_preprocess_model([t_text])\n","#t_text_encode = t_text_prepro.apply(lambda x: bert_model(x)[\"pooled_output\"].numpy())\n","#t_text_encode.to_csv('test_encode.csv', index=False)\n","#print(\"Completed\")"],"metadata":{"id":"Sg2IFHVdOpAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pre = pd.read_csv(\"train_pre.csv\",header=0)\n","train_pre.dropna(inplace=True)\n","y_train = train_pre['Verdict']\n","train_enc = pd.read_csv(\"train_encode.csv\", header=0)['Text']\n","tmp = []\n","for i in train_enc:\n","    data = np.array(i[2:-2].split(), dtype='float')\n","    tmp.append(data)\n","train_enc = pd.DataFrame(tmp)\n","test_pre = pd.read_csv(\"test_pre.csv\",header=0)\n","test_pre.dropna(inplace=True)\n","y_test = test_pre['Verdict']\n","test_enc = pd.read_csv(\"test_encode.csv\", header=0)['Text']\n","tmp = []\n","for i in test_enc:\n","    data = np.array(i[2:-2].split(), dtype='float')\n","    tmp.append(data)\n","test_enc = pd.DataFrame(tmp)"],"metadata":{"id":"PmPd-3Fw8I4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score, precision_recall_fscore_support\n","\n","clf5 = LogisticRegression(C=40, solver='lbfgs',max_iter=5000)\n","clf5.fit(train_enc, y_train)\n","predictions5 = clf5.predict(test_enc)\n","score5 = precision_recall_fscore_support(y_test, predictions5, average='macro')\n","print (score5, clf5.score(test_enc, y_test))"],"metadata":{"id":"iy1OCjgIgI9j"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"4248_Project_bert.ipynb","provenance":[],"collapsed_sections":["TsMoYlUmKqCm","I9ev5FI877TN"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}