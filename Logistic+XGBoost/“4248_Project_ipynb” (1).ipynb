{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (3.2.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (1.8.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (2.0.6)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (0.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (1.22.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (3.0.3)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (1.0.6)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (1.0.1)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (2.4.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (3.0.6)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (8.0.15)\r\n",
      "Requirement already satisfied: setuptools in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (57.0.0)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (0.7.6)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (2.27.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (21.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (4.63.0)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (0.4.0)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (2.0.6)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (0.9.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.7)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting en-core-web-sm==3.2.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 13.9 MB 569 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.3)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.3)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\r\n",
      "Requirement already satisfied: jinja2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.0.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Read Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Read files\n",
    "train = pd.read_csv(\"./original dataset/fulltrain.csv\",header=None)\n",
    "test = pd.read_csv(\"./original dataset/balancedtest.csv\",header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EPSmbFtT2egC"
   },
   "outputs": [],
   "source": [
    "train.columns = ['Verdict','Text']\n",
    "test.columns = ['Verdict','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9lcxgBWiu5zG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3    17870\n1    14047\n4     9995\n2     6942\nName: Verdict, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unbalance training data\n",
    "train['Verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "1    750\n2    750\n3    750\n4    750\nName: Verdict, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Balance test data\n",
    "test['Verdict'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train1=train.copy()\n",
    "test1 = test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y=train['Verdict']\n",
    "X=train['Text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "id": "HTnWi4S_4RkI"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_test_p = pd.DataFrame(X_test)\n",
    "y_test_p = pd.DataFrame(y_test)"
   ],
   "metadata": {
    "id": "mJzm0l-G4oNx"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_small = pd.concat([X_test_p,y_test_p],axis=1)\n",
    "data_small"
   ],
   "metadata": {
    "id": "jAcjWrTS5WKQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "outputId": "41f25cdd-1573-44b5-8166-03bbcc3056bf"
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Text  Verdict\n14019  According to witnesses at your old high school...        1\n44568  The National Immigration Agency (NIA) announce...        4\n43613  The enforcement division of the Securities and...        4\n2213   In an effort to bolster its flagging ratings, ...        1\n17804  Sarah Palin Makes Tragically Heartbreaking New...        2\n...                                                  ...      ...\n3504   Back to story: Netflix Board Of Directors Meet...        1\n28162  Germany Bans Fracking But Theres a HitchBy Bra...        3\n43123  They are forever shocked to be visited in thei...        4\n14471  WOW: Secret Hillary Video Leaks... She Just Lo...        2\n18393  Ted Cruz Absolutely Humiliates Michelle Obama ...        2\n\n[4886 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Verdict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14019</th>\n      <td>According to witnesses at your old high school...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44568</th>\n      <td>The National Immigration Agency (NIA) announce...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>43613</th>\n      <td>The enforcement division of the Securities and...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2213</th>\n      <td>In an effort to bolster its flagging ratings, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17804</th>\n      <td>Sarah Palin Makes Tragically Heartbreaking New...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3504</th>\n      <td>Back to story: Netflix Board Of Directors Meet...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28162</th>\n      <td>Germany Bans Fracking But Theres a HitchBy Bra...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>43123</th>\n      <td>They are forever shocked to be visited in thei...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14471</th>\n      <td>WOW: Secret Hillary Video Leaks... She Just Lo...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18393</th>\n      <td>Ted Cruz Absolutely Humiliates Michelle Obama ...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>4886 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "data_small_X = data_small['Text']\n",
    "data_small_y = data_small['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test_X = test['Text']\n",
    "test_y = test['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbQ7wMzuZI-9"
   },
   "source": [
    "# 3. Preprocessing for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def preprocess_text(s, replace=None, remove_punctuation = None, lower=None,stopword=None,frequency_words=None,scared_word=None, noisy=None, stemming=None,lemmatization=None):\n",
    "    #Throw an error is both stemming and lemmatization are not None\n",
    "\n",
    "    s1 = s.copy()\n",
    "    if stemming is not None and lemmatization is not None:\n",
    "        raise ValueError('Stemming and Lemmatization cannot both be not None!')\n",
    "\n",
    "\n",
    "    if replace is not None:\n",
    "        #Replace URLs with 'webaddress'\n",
    "        s1['Text'] = s1['Text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress',regex=True)\n",
    "        #Replace email address with 'email'\n",
    "        s1['Text'] = s1['Text'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress',regex=True)\n",
    "        #Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "        s1['Text'] = s1['Text'].str.replace(r'£|\\$', 'moneysymb',regex=True)\n",
    "\n",
    "        #Replace percentage symbols with 'percentage'\n",
    "        s1['Text'] = s1['Text'].str.replace(r'%', 'percentage',regex=True)\n",
    "\n",
    "        #Replace 10 digit phone number\n",
    "        s1['Text'] = s1['Text'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr',regex=True)\n",
    "        # Replace numbers with 'numbr'\n",
    "        s1['Text'] = s1['Text'].str.replace(r'\\d+(\\.\\d+)?', 'numbr',regex=True)\n",
    "    #Remove punctuation\n",
    "    if remove_punctuation is not None:\n",
    "        s1['Text'] = s1['Text'].apply(lambda x: re.sub(r'[^\\w\\s\\d]', '', x))\n",
    "\n",
    "    #Transform to lower letter\n",
    "    if lower is not None:\n",
    "        s1['Text'] = s1['Text'].apply(lambda x: x.lower())\n",
    "\n",
    "    #Remove the stopwords\n",
    "    if stopword is not None:\n",
    "        stop=stopwords.words('english')\n",
    "        s1['Text']=s1['Text'].apply(lambda sen:\" \".join(x for x in sen.split() if x not in stop))\n",
    "\n",
    "    #Remove the frequency words\n",
    "    if frequency_words is not None:\n",
    "        freq=pd.Series(' '.join(s).split()).value_counts()[:10]\n",
    "        freq=list(freq.index)\n",
    "        s1['Text']=s1['Text'].apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))\n",
    "\n",
    "    # Remove the scarce word\n",
    "    if scared_word is not None:\n",
    "        scared = pd.Series(' '.join(s).split()).value_counts()[-10:]\n",
    "        scared = list(scared.index)\n",
    "        s1['Text'] = s1['Text'].apply(lambda sen: \" \".join(x for x in sen.split() if x not in scared))\n",
    "\n",
    "    #Noisy Removal\n",
    "    if noisy is not None:\n",
    "        #remove non-ascii\n",
    "        s1['Text']= s1['Text'].apply(lambda x: re.sub(\"(\\\\W)\",\" \",x))\n",
    "        #remove whitespace\n",
    "        s1['Text']=s1['Text'].apply(lambda x: x.strip())\n",
    "\n",
    "    #Stemming\n",
    "    if stemming is not None:\n",
    "        ps = PorterStemmer()\n",
    "        s1['Text']=s1['Text'].apply(lambda x:\" \".join(ps.stem(word) for word in x.split()))\n",
    "\n",
    "    #Lemmatization\n",
    "    if lemmatization is not None:\n",
    "        nltk.download('wordnet')\n",
    "        s1['Text']= s1['Text'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    return s1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Feature Engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Combine three feature engineering methods into one class\n",
    "def feature_engineering(s, train=None,tf_idf=None, word2vec=None, word_count=None):\n",
    "    #1. TF-IDF\n",
    "    s1 = s.copy()\n",
    "    if tf_idf is not None:\n",
    "        tfv = TfidfVectorizer(min_df=3,  max_features=None,strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n",
    "        # Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "        tfv.fit(list(train['Text']))\n",
    "        features =  tfv.transform(s1['Text'])\n",
    "    #2. Word2Vec\n",
    "    if word2vec is not None:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        features = []\n",
    "        for sentence in s1['Text']:\n",
    "            doc = nlp(sentence)\n",
    "            features.append(doc.vector)\n",
    "    #3. Word-count document\n",
    "    if word_count is not None:\n",
    "        #Instantiate the vectorizer\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        features = count_vectorizer.fit_transform(s1['Text'])\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Data Preprocessing (no data preprocessing vs data preprocessing)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6564849626947871, 0.6763333333333333, 0.7131907443613872, 0.6763333333333333]]\n",
      "[[0.6640057322935629, 0.6896666666666667, 0.7238802241381181, 0.6896666666666667]]\n",
      "[[0.6613423276026961, 0.687, 0.7194724199670502, 0.687]]\n",
      "[[0.6613423276026961, 0.687, 0.7194724199670502, 0.687]]\n",
      "[[0.6613423276026961, 0.687, 0.7194724199670502, 0.687]]\n",
      "[[0.6714395719827623, 0.695, 0.7302885119037538, 0.6950000000000001]]\n"
     ]
    }
   ],
   "source": [
    "# Preproecssing\n",
    "# pr1 = no data preprocessing\n",
    "# pre2 = replace, remove_punctionation, lower\n",
    "# pre3 = replace, remove_punctionation, lower, stopword\n",
    "# pre4 = replace, remove_punctionation, lower, stopword, noisy\n",
    "# pre5 = replace, remove_punctionation, lower, stopword, noisy, frequency_words, scared_word\n",
    "# pre6 = replace, remove_punctionation, lower, stopword, noisy, frequency_words, scared_word, lemmatization\n",
    "\n",
    "pre1_train = preprocess_text(data_small)\n",
    "pre1_test = preprocess_text(test)\n",
    "pre2_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1)\n",
    "pre2_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1)\n",
    "pre3_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1)\n",
    "pre3_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1)\n",
    "pre4_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1)\n",
    "pre4_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1)\n",
    "pre5_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1)\n",
    "pre5_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1)\n",
    "pre6_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "\n",
    "pre_train = [(pre1_train,pre1_test),(pre2_train,pre2_test), (pre3_train,pre3_test), (pre4_train,pre4_test), (pre5_train,pre5_test),(pre6_train,pre6_test)]\n",
    "\n",
    "for i in pre_train:\n",
    "    train=i[0]\n",
    "    test=i[1]\n",
    "    train_tf = feature_engineering(train, tf_idf=1, train=train, word2vec=None, word_count=None)\n",
    "    test_tf = feature_engineering(test, tf_idf=1, train=train, word2vec=None, word_count=None)\n",
    "    clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
    "    clf.fit(train_tf, train['Verdict'])\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    y_test=test['Verdict']\n",
    "    score = []\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "    print(score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can find that pre6 can get the best score. (pre6 = replace, remove_punctionation, lower, stopword, noisy, frequency_words, scared_word, lemmatization)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Difference of feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pre6_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "# TF-IDF\n",
    "train_tf = feature_engineering(pre6_train, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
    "test_tf = feature_engineering(pre6_test, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
    "#Word2Vec\n",
    "train_w2v = feature_engineering(pre6_train, tf_idf=None, train=pre6_train, word2vec=1, word_count=None)\n",
    "test_w2v = feature_engineering(pre6_test, tf_idf=None, train=pre6_train, word2vec=1, word_count=None)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6714395719827623, 0.695, 0.7302885119037538, 0.6950000000000001]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
    "clf.fit(train_tf, pre6_train['Verdict'])\n",
    "y_pred = clf.predict(test_tf)\n",
    "y_test=pre6_test['Verdict']\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5311481237730575, 0.5593333333333333, 0.5651206138812424, 0.5593333333333333]]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec\n",
    "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
    "clf.fit(train_w2v, pre6_train['Verdict'])\n",
    "y_pred = clf.predict(test_w2v)\n",
    "y_test=pre6_test['Verdict']\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#LIWC\n",
    "LIWC_train = pd.read_csv(\"./LIWC/df_train_spacy.csv\")\n",
    "LIWC_test = pd.read_csv(\"./LIWC/df_test_spacy.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "train_y = LIWC_train['Verdict']\n",
    "test_y = LIWC_test['Verdict']\n",
    "LIWC_train1 = LIWC_train.copy()\n",
    "LIWC_test1 = LIWC_test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "LIWC_train1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "LIWC_test1.drop(columns=['Verdict','Text'],inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_X = LIWC_train1\n",
    "test_X = LIWC_test1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#Normalization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_X_n = preprocessing.scale(train_X)\n",
    "test_X_n = preprocessing.scale(test_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.484346249502936, 0.497, 0.4825725560197986, 0.497]]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
    "clf.fit(train_X_n, train_y)\n",
    "y_pred = clf.predict(test_X_n)\n",
    "y_test=test_y\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF + LIWC\n",
    "\n",
    "pre6_train = preprocess_text(train1, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "# TF-IDF\n",
    "train_tf_all = feature_engineering(pre6_train, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
    "test_tf_all = feature_engineering(pre6_test, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "train_X_n_m = csr_matrix(train_X_n)\n",
    "train_X_total = hstack([train_tf_all,train_X_n_m])\n",
    "test_X_n_m = csr_matrix(test_X_n)\n",
    "test_X_total = hstack([test_tf_all,test_X_n_m])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5127051017140399, 0.5283333333333333, 0.5072360453567527, 0.5283333333333333]]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
    "clf.fit(train_X_total, train_y)\n",
    "y_pred = clf.predict(test_X_total)\n",
    "y_test=test_y\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Hyperparameters Tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7172891831446663, 0.7168695302792686, 0.7174449066211318, 0.7168785245858303]\n"
     ]
    }
   ],
   "source": [
    "C = [45,60,70,80]\n",
    "result = []\n",
    "for i in C:\n",
    "    clf = LogisticRegression(C=i,max_iter=1000)\n",
    "    clf.fit(train_tf, pre6_train['Verdict'])\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    y_test=pre6_test['Verdict']\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alina/Desktop/NUS/semster4/CS5344/Project Handout and Readings/pythonProject/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.716091381323241, 0.7172891831446663, 0.7172891831446663, 0.7172891831446663, 0.7172891831446663, 0.7172891831446663]\n"
     ]
    }
   ],
   "source": [
    "max_iter = [100,500,1000,1500,2000,3000]\n",
    "result = []\n",
    "for i in max_iter:\n",
    "    clf = LogisticRegression(C=40,max_iter=i)\n",
    "    clf.fit(train_tf, pre6_train['Verdict'])\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    y_test=pre6_test['Verdict']\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7172891831446663, 0.7164293296287682, 0.716531525532803, 0.7172891831446663]\n"
     ]
    }
   ],
   "source": [
    "solver = ['newton-cg','sag','saga','lbfgs']\n",
    "result = []\n",
    "for i in solver:\n",
    "    clf = LogisticRegression(C=40,max_iter=500, solver=i)\n",
    "    clf.fit(train_tf, pre6_train['Verdict'])\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    y_test=pre6_test['Verdict']\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 Final Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.1 TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Hyperparameter tunning\n",
    "\n",
    "pre6_train = preprocess_text(train1, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "train_y = pre6_train['Verdict']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(pre6_train,train_y,test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "train_tf_all = feature_engineering(X_train1, tf_idf=1, train=X_train1, word2vec=None, word_count=None)\n",
    "test_tf_all = feature_engineering(X_test1, tf_idf=1, train=X_train1, word2vec=None, word_count=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_y = X_train1['Verdict']\n",
    "y_test = X_test1['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9696072297193579, 0.9710367413775458, 0.9722313715712269, 0.9671314555427126]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "lg = LogisticRegression(C=40,max_iter=1000,solver='newton-cg')\n",
    "lg.fit(train_tf_all, y_train1)\n",
    "y_pred = lg.predict(test_tf_all)\n",
    "score = []\n",
    "score2 = []\n",
    "f1_macro = f1_score(y_test1, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "precision_macro = precision_score(y_test1, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test1, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999443840384346\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(C=40,max_iter=1000,solver='newton-cg')\n",
    "lg.fit(train_tf_all, y_train1)\n",
    "y_pred = lg.predict(train_tf_all)\n",
    "f1_macro2 = f1_score(y_train1, y_pred, average='macro')\n",
    "print(f1_macro2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7489222716729435, 0.7573333333333333, 0.7737924783133107, 0.7573333333333333]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "lg = LogisticRegression(C=50,max_iter=1500,solver='newton-cg')\n",
    "lg.fit(train_tf_all, train_y)\n",
    "y_pred = lg.predict(test_tf_all)\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7493149510808985, 0.758, 0.7745434572248238, 0.758]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "lg = LogisticRegression(C=40,max_iter=1000,solver='newton-cg')\n",
    "lg.fit(train_tf_all, train_y)\n",
    "y_pred = lg.predict(test_tf_all)\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.2 TF-IDF + LIWC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train2 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m      2\u001B[0m test2 \u001B[38;5;241m=\u001B[39m test1\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train1' is not defined"
     ]
    }
   ],
   "source": [
    "train2 = train1.copy()\n",
    "test2 = test1.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "pre6_train = preprocess_text(train2, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test = preprocess_text(test2, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "y_train = pre6_train['Verdict']\n",
    "y_test = pre6_test['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "train_tf = feature_engineering(pre6_train, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
    "test_tf = feature_engineering(pre6_test, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#LIWC\n",
    "LIWC_train = pd.read_csv(\"./LIWC/df_train_spacy.csv\")\n",
    "LIWC_test = pd.read_csv(\"./LIWC/df_test_spacy.csv\")\n",
    "\n",
    "train_y = LIWC_train['Verdict']\n",
    "test_y = LIWC_test['Verdict']\n",
    "LIWC_train1 = LIWC_train.copy()\n",
    "LIWC_test1 = LIWC_test.copy()\n",
    "LIWC_train1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "LIWC_test1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "train_X = LIWC_train1\n",
    "test_X = LIWC_test1\n",
    "#Normalization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_X_n = preprocessing.scale(train_X)\n",
    "test_X_n = preprocessing.scale(test_X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(45854, 114)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIWC_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(3000, 114)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIWC_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "train_X_n_m = csr_matrix(train_X_n)\n",
    "train_X_total = hstack([train_tf,train_X_n_m])\n",
    "test_X_n_m = csr_matrix(test_X_n)\n",
    "test_X_total = hstack([test_tf,test_X_n_m])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5438564372888904, 0.5466666666666666, 0.5425634864321488, 0.5466666666666666]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "lg = LogisticRegression(C=0.01,max_iter=1000,solver='newton-cg')\n",
    "lg.fit(train_X_total, train_y)\n",
    "y_pred = lg.predict(test_X_total)\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "train2=train.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "test2 = test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "(9771, 844239)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_total.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train3 = train2.sample(frac=1.0, random_state=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(48854, 2)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877309693722454\n"
     ]
    }
   ],
   "source": [
    "train3 = train2.sample(frac=1.0, random_state=10)\n",
    "pre6_train = preprocess_text(train3, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "# pre6_test = preprocess_text(test3, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "train_y = pre6_train['Verdict']\n",
    "X_train = train3[0:45854]\n",
    "X_test = train3[45854:48854]\n",
    "y_train = X_train['Verdict']\n",
    "y_test = X_test['Verdict']\n",
    "\n",
    "# TF-IDF\n",
    "train_tf = feature_engineering(X_train, tf_idf=1, train=X_train, word2vec=None, word_count=None)\n",
    "test_tf = feature_engineering(X_test, tf_idf=1, train=X_train, word2vec=None, word_count=None)\n",
    "\n",
    "\n",
    "#LIWC\n",
    "LIWC_all = pd.read_csv(\"./LIWC/df_train_spacy.csv\")\n",
    "LIWC_all2 = LIWC_all.sample(frac=1.0, random_state=10)\n",
    "LIWC_train = LIWC_all2[0:45854]\n",
    "LIWC_test = LIWC_all2[45854:48854]\n",
    "LIWC_train1 = LIWC_train.copy()\n",
    "LIWC_test1 = LIWC_test.copy()\n",
    "LIWC_y = LIWC_train['Verdict']\n",
    "LIWC_y = LIWC_test['Verdict']\n",
    "LIWC_train1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "LIWC_test1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "train_X = LIWC_train1\n",
    "test_X = LIWC_test1\n",
    "\n",
    "#Normalization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_X_n = preprocessing.scale(train_X)\n",
    "test_X_n = preprocessing.scale(test_X)\n",
    "\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "train_X_n_m = csr_matrix(train_X_n)\n",
    "train_X_total = hstack([train_tf,train_X_n_m])\n",
    "test_X_n_m = csr_matrix(test_X_n)\n",
    "test_X_total = hstack([test_tf,test_X_n_m])\n",
    "train_y = X_train['Verdict']\n",
    "y_test = X_test['Verdict']\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "lg = LogisticRegression(C=0.01,max_iter=1000,solver='newton-cg')\n",
    "lg.fit(train_X_total, train_y)\n",
    "y_pred = lg.predict(train_X_total)\n",
    "f1_macro = f1_score(train_y, y_pred, average='macro')\n",
    "print(f1_macro)\n",
    "\n",
    "\n",
    "# # from sklearn.model_selection import GridSearchCV\n",
    "# lg = LogisticRegression(C=0.01,max_iter=1000,solver='newton-cg')\n",
    "# lg.fit(train_X_total, train_y)\n",
    "# y_pred = lg.predict(test_X_total)\n",
    "# score = []\n",
    "# f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "# recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "# score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "# print(score)\n",
    "\n",
    "\n",
    "# # TF-IDF\n",
    "# train_tf = feature_engineering(pre6_train, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
    "# test_tf = feature_engineering(pre6_test, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# #LIWC\n",
    "# LIWC_train = pd.read_csv(\"./LIWC/df_train_spacy.csv\")\n",
    "# # LIWC_test = pd.read_csv(\"./LIWC/df_test_spacy.csv\")\n",
    "#\n",
    "# train_y = LIWC_train['Verdict']\n",
    "# test_y = LIWC_test['Verdict']\n",
    "# LIWC_train1 = LIWC_train.copy()\n",
    "# LIWC_test1 = LIWC_test.copy()\n",
    "# LIWC_train1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "# LIWC_test1.drop(columns=['Verdict','Text'],inplace=True)\n",
    "# train_X = LIWC_train1\n",
    "# test_X = LIWC_test1\n",
    "# #Normalization\n",
    "# from sklearn import preprocessing\n",
    "#\n",
    "# train_X_n = preprocessing.scale(train_X)\n",
    "# test_X_n = preprocessing.scale(test_X)\n",
    "#\n",
    "#\n",
    "# # from sklearn.decomposition import PCA\n",
    "# # pca = PCA(n_components=50)\n",
    "# # pca_train_X = pca.fit_transform(train_X_n)\n",
    "# # pca_test_X = pca.fit_transform(test_X_n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "train_X_n_m = csr_matrix(train_X_n)\n",
    "train_X_total = hstack([train_tf,train_X_n_m])\n",
    "test_X_n_m = csr_matrix(test_X_n)\n",
    "test_X_total = hstack([test_tf,test_X_n_m])\n",
    "train_y=train1['Verdict']\n",
    "test_y = test['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#Randomly choose 10% dataset from the original one\n",
    "X_train, X_one_ten, y_train, y_one_ten = train_test_split(train_X_total, train_y, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41105864096998207]\n"
     ]
    }
   ],
   "source": [
    "C = [20.0]\n",
    "result = []\n",
    "for i in C:\n",
    "    clf = LogisticRegression(C=i,max_iter=1000)\n",
    "    clf.fit(X_one_ten, y_one_ten)\n",
    "    y_pred = clf.predict(test_X_total)\n",
    "    y_test=test_y\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(activation='logistic', alpha=0.001,learning_rate_init=0.01,early_stopping=True)\n",
    "clf = clf.fit(X_one_ten,y_one_ten)\n",
    "#Perform prediction\n",
    "predictions = clf.predict(test_X_total)\n",
    "y_test=test_y\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 Larger and Smaller dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "#Read files\n",
    "train_smaller = pd.read_csv(\"smaller dataset/samlltraining.csv\")\n",
    "test_smaller = pd.read_csv(\"smaller dataset/balancedtest.csv\",header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "      Verdict                                               Text\n0           1  When so many actors seem content to churn out ...\n1           1   In what football insiders are calling an unex...\n2           1  In a freak accident following Game 3 of the N....\n3           1  North Koreas official news agency announced to...\n4           1  The former Alaska Governor Sarah Palin would b...\n...       ...                                                ...\n2995        4  The Air Force mistakenly gave rival companies ...\n2996        4  The United Nations climate chief on Friday cha...\n2997        4  River Plate midfielder Diego Buonanotte has un...\n2998        4  Lawmakers were on the brink Tuesday of exempti...\n2999        4  The Pentagon, which is processing bids on a ne...\n\n[3000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Verdict</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>When so many actors seem content to churn out ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>In what football insiders are calling an unex...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>In a freak accident following Game 3 of the N....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>North Koreas official news agency announced to...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>The former Alaska Governor Sarah Palin would b...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>4</td>\n      <td>The Air Force mistakenly gave rival companies ...</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>4</td>\n      <td>The United Nations climate chief on Friday cha...</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>4</td>\n      <td>River Plate midfielder Diego Buonanotte has un...</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>4</td>\n      <td>Lawmakers were on the brink Tuesday of exempti...</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>4</td>\n      <td>The Pentagon, which is processing bids on a ne...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_smaller.columns = ['Verdict','Text']\n",
    "test_smaller"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Hyperparameter tunning\n",
    "\n",
    "pre6_train_s = preprocess_text(train_smaller, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test_s = preprocess_text(test_smaller, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "# TF-IDF\n",
    "train_tf_all_s = feature_engineering(train_smaller, tf_idf=1, train=train_smaller, word2vec=None, word_count=None)\n",
    "test_tf_all_s = feature_engineering(test_smaller, tf_idf=1, train=train_smaller, word2vec=None, word_count=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "train_y_s = pre6_train_s['Verdict']\n",
    "y_test_s = pre6_test_s['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7132416005458351, 0.7206666666666667, 0.7387821344926073, 0.7206666666666667]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "lg = LogisticRegression(C=40,max_iter=1000,solver='newton-cg')\n",
    "lg.fit(train_tf_all_s, train_y_s)\n",
    "y_pred = lg.predict(test_tf_all_s)\n",
    "score = []\n",
    "f1_macro = f1_score(y_test_s, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test_s, y_pred)\n",
    "precision_macro = precision_score(y_test_s, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test_s, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pre6_train = preprocess_text(train1, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "pre6_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
    "# TF-IDF\n",
    "train_tf = feature_engineering(pre6_train, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
    "test_tf = feature_engineering(pre6_test, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_y = pre6_train['Verdict']\n",
    "y_test = pre6_test['Verdict']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#Randomly choose 10% dataset from the original one\n",
    "X_train, X_one_ten, y_train, y_one_ten = train_test_split(train_tf, train_y, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5623041209433175, 0.5726666666666667, 0.5799341825673994, 0.5726666666666667]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "xgb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "xgb.fit(X_one_ten, y_one_ten)\n",
    "y_pred = xgb.predict(test_tf)\n",
    "score = []\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Hyperparameter Tunning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2355592789585601, 0.11202363899920233]\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate\n",
    "lr = [5,10]\n",
    "result = []\n",
    "for i in lr:\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=i, max_depth=1, random_state=0)\n",
    "    clf.fit(X_one_ten, y_one_ten)\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5924594859660719]\n"
     ]
    }
   ],
   "source": [
    "#Learning Rate\n",
    "# n_estimators = [100,200,500]\n",
    "n_estimators = [1000]\n",
    "result = []\n",
    "for i in n_estimators:\n",
    "    clf = GradientBoostingClassifier(n_estimators=i, learning_rate=1, max_depth=1, random_state=0)\n",
    "    clf.fit(X_one_ten, y_one_ten)\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5892325989335394, 0.5902830290613534, 0.5950372882525081, 0.6016877979796372]\n"
     ]
    }
   ],
   "source": [
    "#Max Depth\n",
    "max_depth = [1,2,3,5]\n",
    "result = []\n",
    "for i in max_depth:\n",
    "    clf = GradientBoostingClassifier(n_estimators=500, learning_rate=1, max_depth=i, random_state=0)\n",
    "    clf.fit(X_one_ten, y_one_ten)\n",
    "    y_pred = clf.predict(test_tf)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    result.append(f1_macro)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "“4248_Project.ipynb”",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}