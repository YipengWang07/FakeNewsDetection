{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5S7DYEfkZl_-"
      },
      "source": [
        "# 1. Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F7UTzrPZmAA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import scipy\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from textblob import Word\n",
        "from sklearn.utils import resample\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix,hstack\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd4Ku1LKZmAB",
        "outputId": "19442a56-41cc-47b2-f085-f46049a05225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUdnDRI-ZmAC",
        "outputId": "caf2cb7c-3cd6-41a0-a9d4-6643562eb5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip3 install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Qhb7SSV1ZmAD"
      },
      "source": [
        "# 2. Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bek3azPgZ_iG",
        "outputId": "6b2dbfa6-d2b3-4a3a-ca03-346e859d605f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oScesYTZmAD"
      },
      "outputs": [],
      "source": [
        "#Read files\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/raw_data/fulltrain.csv\",header=None)\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/raw_data/balancedtest.csv\",header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPSmbFtT2egC"
      },
      "outputs": [],
      "source": [
        "train.columns = ['Verdict','Text']\n",
        "test.columns = ['Verdict','Text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lcxgBWiu5zG",
        "outputId": "cac83733-9c1f-4749-bca0-d6f903a1e935"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    17870\n",
              "1    14047\n",
              "4     9995\n",
              "2     6942\n",
              "Name: Verdict, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Unbalance training data\n",
        "train['Verdict'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bglKC42ZmAE"
      },
      "outputs": [],
      "source": [
        "train1=train.copy()\n",
        "test1 = test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTnWi4S_4RkI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y=train['Verdict']\n",
        "X=train['Text']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJzm0l-G4oNx"
      },
      "outputs": [],
      "source": [
        "X_test_p = pd.DataFrame(X_test)\n",
        "y_test_p = pd.DataFrame(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jAcjWrTS5WKQ",
        "outputId": "70f6c3fc-fd53-4994-ea6f-92b0e14ff748"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-10b36ccf-bfe1-481c-ace2-7cd65ae86c31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Verdict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14019</th>\n",
              "      <td>According to witnesses at your old high school...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44568</th>\n",
              "      <td>The National Immigration Agency (NIA) announce...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43613</th>\n",
              "      <td>The enforcement division of the Securities and...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>In an effort to bolster its flagging ratings, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17804</th>\n",
              "      <td>Sarah Palin Makes Tragically Heartbreaking New...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>Back to story: Netflix Board Of Directors Meet...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28162</th>\n",
              "      <td>Germany Bans Fracking But Theres a HitchBy Bra...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43123</th>\n",
              "      <td>They are forever shocked to be visited in thei...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14471</th>\n",
              "      <td>WOW: Secret Hillary Video Leaks... She Just Lo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18393</th>\n",
              "      <td>Ted Cruz Absolutely Humiliates Michelle Obama ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4886 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10b36ccf-bfe1-481c-ace2-7cd65ae86c31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10b36ccf-bfe1-481c-ace2-7cd65ae86c31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10b36ccf-bfe1-481c-ace2-7cd65ae86c31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    Text  Verdict\n",
              "14019  According to witnesses at your old high school...        1\n",
              "44568  The National Immigration Agency (NIA) announce...        4\n",
              "43613  The enforcement division of the Securities and...        4\n",
              "2213   In an effort to bolster its flagging ratings, ...        1\n",
              "17804  Sarah Palin Makes Tragically Heartbreaking New...        2\n",
              "...                                                  ...      ...\n",
              "3504   Back to story: Netflix Board Of Directors Meet...        1\n",
              "28162  Germany Bans Fracking But Theres a HitchBy Bra...        3\n",
              "43123  They are forever shocked to be visited in thei...        4\n",
              "14471  WOW: Secret Hillary Video Leaks... She Just Lo...        2\n",
              "18393  Ted Cruz Absolutely Humiliates Michelle Obama ...        2\n",
              "\n",
              "[4886 rows x 2 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_small = pd.concat([X_test_p,y_test_p],axis=1)\n",
        "data_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLljOEr4ZmAF"
      },
      "outputs": [],
      "source": [
        "data_small_X = data_small['Text']\n",
        "data_small_y = data_small['Verdict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_QLTwTtZmAF"
      },
      "outputs": [],
      "source": [
        "test_X = test['Text']\n",
        "test_y = test['Verdict']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbQ7wMzuZI-9"
      },
      "source": [
        "# 3. Preprocessing for Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZtbfgYTZmAG"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(s, replace=None, remove_punctuation = None, lower=None,stopword=None,frequency_words=None,scared_word=None, noisy=None, stemming=None,lemmatization=None):\n",
        "    #Throw an error is both stemming and lemmatization are not None\n",
        "\n",
        "    s1 = s.copy()\n",
        "    if stemming is not None and lemmatization is not None:\n",
        "        raise ValueError('Stemming and Lemmatization cannot both be not None!')\n",
        "\n",
        "\n",
        "    if replace is not None:\n",
        "        #Replace URLs with 'webaddress'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
        "                                  'webaddress',regex=True)\n",
        "        #Replace email address with 'email'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
        "                                 'emailaddress',regex=True)\n",
        "        #Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
        "        s1['Text'] = s1['Text'].str.replace(r'£|\\$', 'moneysymb',regex=True)\n",
        "\n",
        "        #Replace percentage symbols with 'percentage'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'%', 'percentage',regex=True)\n",
        "\n",
        "        #Replace 10 digit phone number\n",
        "        s1['Text'] = s1['Text'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
        "                                  'phonenumbr',regex=True)\n",
        "        # Replace numbers with 'numbr'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'\\d+(\\.\\d+)?', 'numbr',regex=True)\n",
        "    #Remove punctuation\n",
        "    if remove_punctuation is not None:\n",
        "        s1['Text'] = s1['Text'].apply(lambda x: re.sub(r'[^\\w\\s\\d]', '', x))\n",
        "\n",
        "    #Transform to lower letter\n",
        "    if lower is not None:\n",
        "        s1['Text'] = s1['Text'].apply(lambda x: x.lower())\n",
        "\n",
        "    #Remove the stopwords\n",
        "    if stopword is not None:\n",
        "        stop=stopwords.words('english')\n",
        "        s1['Text']=s1['Text'].apply(lambda sen:\" \".join(x for x in sen.split() if x not in stop))\n",
        "\n",
        "    #Remove the frequency words\n",
        "    if frequency_words is not None:\n",
        "        freq=pd.Series(' '.join(s).split()).value_counts()[:10]\n",
        "        freq=list(freq.index)\n",
        "        s1['Text']=s1['Text'].apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))\n",
        "\n",
        "    # Remove the scarce word\n",
        "    if scared_word is not None:\n",
        "        scared = pd.Series(' '.join(s).split()).value_counts()[-10:]\n",
        "        scared = list(scared.index)\n",
        "        s1['Text'] = s1['Text'].apply(lambda sen: \" \".join(x for x in sen.split() if x not in scared))\n",
        "\n",
        "    #Noisy Removal\n",
        "    if noisy is not None:\n",
        "        #remove non-ascii\n",
        "        s1['Text']= s1['Text'].apply(lambda x: re.sub(\"(\\\\W)\",\" \",x))\n",
        "        #remove whitespace\n",
        "        s1['Text']=s1['Text'].apply(lambda x: x.strip())\n",
        "\n",
        "    #Stemming\n",
        "    if stemming is not None:\n",
        "        ps = PorterStemmer()\n",
        "        s1['Text']=s1['Text'].apply(lambda x:\" \".join(ps.stem(word) for word in x.split()))\n",
        "\n",
        "    #Lemmatization\n",
        "    if lemmatization is not None:\n",
        "        nltk.download('wordnet')\n",
        "        s1['Text']= s1['Text'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "    return s1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-m5phJ4lZmAH"
      },
      "source": [
        "# 4. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOqIKB9RZmAH"
      },
      "outputs": [],
      "source": [
        "#Combine three feature engineering methods into one class\n",
        "def feature_engineering(s, train=None,tf_idf=None, word2vec=None, word_count=None):\n",
        "    #1. TF-IDF\n",
        "    s1 = s.copy()\n",
        "    if tf_idf is not None:\n",
        "        tfv = TfidfVectorizer(min_df=3,  max_features=None,strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n",
        "        # Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
        "        tfv.fit(list(train['Text']))\n",
        "        features =  tfv.transform(s1['Text'])\n",
        "    #2. Word2Vec\n",
        "    if word2vec is not None:\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        features = []\n",
        "        for sentence in s1['Text']:\n",
        "            doc = nlp(sentence)\n",
        "            features.append(doc.vector)\n",
        "    #3. Word-count document\n",
        "    if word_count is not None:\n",
        "        #Instantiate the vectorizer\n",
        "        count_vectorizer = CountVectorizer()\n",
        "        features = count_vectorizer.fit_transform(s1['Text'])\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Lj5PYlMoZmAI"
      },
      "source": [
        "# 5. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "I-YoYOcMZmAI"
      },
      "source": [
        "## 5.1 Data Preprocessing (no data preprocessing vs data preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxa-Zf45ZmAI",
        "outputId": "e47cd8a1-c469-4b3d-89c6-5dbb2e59549f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOclhQhYZmAJ",
        "outputId": "4aedd308-30ec-428c-af5e-997410f9d7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[[0.6564849626947871, 0.6763333333333333, 0.7131907443613872, 0.6763333333333333]]\n",
            "[[0.6640057322935629, 0.6896666666666667, 0.7238802241381181, 0.6896666666666667]]\n",
            "[[0.6613423276026961, 0.687, 0.7194724199670502, 0.687]]\n",
            "[[0.6613423276026961, 0.687, 0.7194724199670502, 0.687]]\n",
            "[[0.6613423276026961, 0.687, 0.7194724199670502, 0.687]]\n",
            "[[0.6714395719827623, 0.695, 0.7302885119037538, 0.6950000000000001]]\n",
            "[[0.6730502196339856, 0.697, 0.7336226717847157, 0.697]]\n"
          ]
        }
      ],
      "source": [
        "# Preproecssing\n",
        "# pr1 = no data preprocessing\n",
        "# pre2 = replace, remove_punctionation, lower\n",
        "# pre3 = replace, remove_punctionation, lower, stopword\n",
        "# pre4 = replace, remove_punctionation, lower, stopword, noisy\n",
        "# pre5 = replace, remove_punctionation, lower, stopword, noisy, frequency_words, scared_word\n",
        "# pre6 = replace, remove_punctionation, lower, stopword, noisy, frequency_words, scared_word, lemmatization\n",
        "# pre7 = replace, remove_punctionation, lower, lemmatization\n",
        "\n",
        "pre1_train = preprocess_text(data_small)\n",
        "pre1_test = preprocess_text(test)\n",
        "pre2_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1)\n",
        "pre2_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1)\n",
        "pre3_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1)\n",
        "pre3_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1)\n",
        "pre4_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1)\n",
        "pre4_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1)\n",
        "pre5_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1)\n",
        "pre5_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1)\n",
        "pre6_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
        "pre6_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
        "pre7_train = preprocess_text(data_small, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "\n",
        "pre_train = [(pre1_train,pre1_test),(pre2_train,pre2_test), (pre3_train,pre3_test), (pre4_train,pre4_test), (pre5_train,pre5_test),(pre6_train,pre6_test),(pre7_train,pre7_test)]\n",
        "\n",
        "for i in pre_train:\n",
        "    train=i[0]\n",
        "    test=i[1]\n",
        "    train_tf = feature_engineering(train, tf_idf=1, train=train, word2vec=None, word_count=None)\n",
        "    test_tf = feature_engineering(test, tf_idf=1, train=train, word2vec=None, word_count=None)\n",
        "    clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
        "    clf.fit(train_tf, train['Verdict'])\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    y_test=test['Verdict']\n",
        "    score = []\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "    score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "    print(score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ABElrCuCZmAJ"
      },
      "source": [
        "We can find that pre7 can get the best score. (pre7 = replace, remove_punctionation, lower, lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EIfQcbh6ZmAJ"
      },
      "source": [
        "## 5.2 Difference of feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwx6GfRZp-q9",
        "outputId": "e8d2e730-526b-44fd-f538-3e036c181e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "pre7_alltrain = preprocess_text(train1, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDAnTaEUZmAJ"
      },
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "train_tf = feature_engineering(pre7_alltrain, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
        "test_tf = feature_engineering(pre7_test, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)\n",
        "#Word2Vec\n",
        "train_w2v = feature_engineering(pre7_alltrain, tf_idf=None, train=pre6_train, word2vec=1, word_count=None)\n",
        "test_w2v = feature_engineering(pre7_test, tf_idf=None, train=pre6_train, word2vec=1, word_count=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovQcMHyPqbIE"
      },
      "source": [
        "### 5.2.1 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KtL3h5RZmAK",
        "outputId": "a1433921-4aba-4c2b-c8c8-921c4e00e973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.7455367922168222, 0.7553333333333333, 0.7728430610239194, 0.7553333333333333]]\n"
          ]
        }
      ],
      "source": [
        "#TF-IDF\n",
        "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
        "clf.fit(train_tf, pre7_alltrain['Verdict'])\n",
        "y_pred = clf.predict(test_tf)\n",
        "y_test=pre7_test['Verdict']\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-puavU86qei-"
      },
      "source": [
        "### 5.2.2 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkTBFNNPZmAK",
        "outputId": "34a26bca-2f7b-4e4b-8ddc-b157057cc882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.548621234271049, 0.5656666666666667, 0.569778908217901, 0.5656666666666667]]\n"
          ]
        }
      ],
      "source": [
        "#Word2Vec\n",
        "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
        "clf.fit(train_w2v, pre7_alltrain['Verdict'])\n",
        "y_pred = clf.predict(test_w2v)\n",
        "y_test=pre7_test['Verdict']\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHBpN3yeqjE2"
      },
      "source": [
        "### 5.2.3 LIWC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXzHco_LZmAK"
      },
      "outputs": [],
      "source": [
        "#LIWC\n",
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_X_train.pickle\", 'rb') as f1:\n",
        "    LIWC_train = pickle.load(f1)\n",
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_X_test.pickle\", 'rb') as f2:\n",
        "    LIWC_test = pickle.load(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zOG_1eriiUX"
      },
      "outputs": [],
      "source": [
        "LIWC_train = pd.DataFrame(LIWC_train)\n",
        "LIWC_test = pd.DataFrame(LIWC_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_iI0mSfiLhp"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_y_train.pickle\", 'rb') as f3:\n",
        "    LIWC_train_y = pickle.load(f3)\n",
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_y_test.pickle\", 'rb') as f4:\n",
        "    LIWC_test_y = pickle.load(f4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yru6cgtgZmAL"
      },
      "outputs": [],
      "source": [
        "train_y = LIWC_train_y\n",
        "test_y = LIWC_test_y\n",
        "LIWC_train1 = LIWC_train.copy()\n",
        "LIWC_test1 = LIWC_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m9L9xJpZmAL"
      },
      "outputs": [],
      "source": [
        "train_X = LIWC_train1.iloc[:,:112]\n",
        "test_X = LIWC_test1.iloc[:,:112]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_aSK9L5ZmAL"
      },
      "outputs": [],
      "source": [
        "#Normalization\n",
        "from sklearn import preprocessing\n",
        "\n",
        "train_X_n = preprocessing.scale(train_X)\n",
        "test_X_n = preprocessing.scale(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojwO_OmhZmAL",
        "outputId": "d36b9b12-434b-4744-bef4-0b8ed6db3250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.476288217731061, 0.48933333333333334, 0.4738564991638031, 0.4893333333333333]]\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
        "clf.fit(train_X_n, train_y)\n",
        "y_pred = clf.predict(test_X_n)\n",
        "y_test=test_y\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_cJbjCkjIHt"
      },
      "source": [
        "#### 5.2.3.1 LIWC + Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfmKU55BjMbd"
      },
      "outputs": [],
      "source": [
        "#LIWC\n",
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_X_train.pickle\", 'rb') as f1:\n",
        "    LIWC_train = pickle.load(f1)\n",
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_X_test.pickle\", 'rb') as f2:\n",
        "    LIWC_test = pickle.load(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_qBNENfjSId"
      },
      "outputs": [],
      "source": [
        "LIWC_train = pd.DataFrame(LIWC_train)\n",
        "LIWC_test = pd.DataFrame(LIWC_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P5nx9F7jVhX"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_y_train.pickle\", 'rb') as f3:\n",
        "    LIWC_train_y = pickle.load(f3)\n",
        "with open(\"/content/drive/MyDrive/CS4248 Project/liwc+glove/mlp_y_test.pickle\", 'rb') as f4:\n",
        "    LIWC_test_y = pickle.load(f4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gUocMePjWpA"
      },
      "outputs": [],
      "source": [
        "train_y = LIWC_train_y\n",
        "test_y = LIWC_test_y\n",
        "LIWC_train1 = LIWC_train.copy()\n",
        "LIWC_test1 = LIWC_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfDZJKZKjXzn"
      },
      "outputs": [],
      "source": [
        "train_X = LIWC_train1\n",
        "test_X = LIWC_test1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Geiaw0YjZrQ"
      },
      "outputs": [],
      "source": [
        "#Normalization\n",
        "from sklearn import preprocessing\n",
        "\n",
        "train_X_n = preprocessing.scale(train_X)\n",
        "test_X_n = preprocessing.scale(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HHMglLrjZ9A",
        "outputId": "c18404e0-5b6b-43bd-b9c7-e984eb7ff67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5239150358155775, 0.532, 0.5213275652189299, 0.532]]\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
        "clf.fit(train_X_n, train_y)\n",
        "y_pred = clf.predict(test_X_n)\n",
        "y_test=test_y\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFpUrZF6jNez"
      },
      "source": [
        "####5.2.3.2 TF-IDF + LIWC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-INUnZIZmAL",
        "outputId": "32924a78-4594-49c9-e0d8-383b9f2cd135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#TF-IDF + LIWC\n",
        "\n",
        "pre7_alltrain = preprocess_text(train1, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "\n",
        "# TF-IDF\n",
        "train_tf_all = feature_engineering(pre7_alltrain, tf_idf=1, train=pre7_alltrain, word2vec=None, word_count=None)\n",
        "test_tf_all = feature_engineering(pre7_test, tf_idf=1, train=pre7_alltrain, word2vec=None, word_count=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGTYGbZQZmAL"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix,hstack\n",
        "train_X = LIWC_train1.iloc[:,:112]\n",
        "test_X = LIWC_test1.iloc[:,:112]\n",
        "train_X_n = preprocessing.scale(train_X)\n",
        "test_X_n = preprocessing.scale(test_X)\n",
        "\n",
        "train_X_n_m = csr_matrix(train_X_n)\n",
        "train_X_total = hstack([train_tf_all,train_X_n_m])\n",
        "test_X_n_m = csr_matrix(test_X_n)\n",
        "test_X_total = hstack([test_tf_all,test_X_n_m])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmENslm4ZmAM",
        "outputId": "7420d08d-4039-406e-fe77-efd7ade67f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5067317016117807, 0.5223333333333333, 0.5012040185318641, 0.5223333333333333]]\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(C=1.0, solver='lbfgs',max_iter=3000)\n",
        "clf.fit(train_X_total, train_y)\n",
        "y_pred = clf.predict(test_X_total)\n",
        "y_test=test_y\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FyNZXBDBZmAM"
      },
      "source": [
        "## 5.3 Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZYnwZTTtLKE"
      },
      "outputs": [],
      "source": [
        "train_smalltf = feature_engineering(pre7_train, tf_idf=1, train=pre6_train, word2vec=None, word_count=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPIvTEl0ZmAM",
        "outputId": "2bee800a-2ee3-401f-ae11-3b3a3d2e0a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7217188724163944, 0.7225850096469508, 0.7229108796366671, 0.7225348534629683]\n"
          ]
        }
      ],
      "source": [
        "C = [45,60,70,80]\n",
        "result = []\n",
        "for i in C:\n",
        "    clf = LogisticRegression(C=i,max_iter=1000)\n",
        "    clf.fit(train_smalltf,pre7_train['Verdict'])\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    y_test=pre7_test['Verdict']\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KicspttauyXj",
        "outputId": "8b13c19c-e420-4c4c-a5b8-f1d97175aa25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1, 0.1, 0.10207209592375097, 0.4572874054654142]\n"
          ]
        }
      ],
      "source": [
        "C = [0.0001,0.001,0.01,0.1]\n",
        "result = []\n",
        "for i in C:\n",
        "    clf = LogisticRegression(C=i,max_iter=1000)\n",
        "    clf.fit(train_smalltf,pre7_train['Verdict'])\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    y_test=pre7_test['Verdict']\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsUwn9vEvRsr",
        "outputId": "f4168c90-4779-492e-8b8c-3d2e2854e6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.6868295517497363, 0.7149693422660267, 0.7207270301536691, 0.7200622736546014]\n"
          ]
        }
      ],
      "source": [
        "C = [1,10,20,30]\n",
        "result = []\n",
        "for i in C:\n",
        "    clf = LogisticRegression(C=i,max_iter=1000)\n",
        "    clf.fit(train_smalltf,pre7_train['Verdict'])\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    y_test=pre7_test['Verdict']\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcg3fuAQZmAM",
        "outputId": "02c8aaf9-55d8-460d-872f-3785f8ebd0e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7208965316973053, 0.7229108796366671, 0.7229108796366671, 0.7229108796366671, 0.7229108796366671, 0.7229108796366671]\n"
          ]
        }
      ],
      "source": [
        "max_iter = [100,500,1000,1500,2000,3000]\n",
        "result = []\n",
        "for i in max_iter:\n",
        "    clf = LogisticRegression(C=70,max_iter=i)\n",
        "    clf.fit(train_smalltf, pre7_train['Verdict'])\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    y_test=pre6_test['Verdict']\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yebal1V_ZmAM",
        "outputId": "b674380b-0a6b-4b17-d8bb-381879c0f61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7229108796366671, 0.7223104254705561, 0.7212449677019634, 0.7229108796366671]\n"
          ]
        }
      ],
      "source": [
        "solver = ['newton-cg','sag','saga','lbfgs']\n",
        "result = []\n",
        "for i in solver:\n",
        "    clf = LogisticRegression(C=70,max_iter=1000, solver=i)\n",
        "    clf.fit(train_smalltf, pre7_train['Verdict'])\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    y_test=pre6_test['Verdict']\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f3tgfzaKZmAN"
      },
      "source": [
        "## 5.4 Final Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RVpAkyAOZmAN"
      },
      "source": [
        "### 5.4.1 TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBqziuEJ4nps"
      },
      "source": [
        "#### 5.4.1.1 Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YBCIjP95aX9"
      },
      "outputs": [],
      "source": [
        "y_test=pre7_test['Verdict']\n",
        "y_train=pre7_alltrain['Verdict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cAPwGMXE-wx"
      },
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "train_tf = feature_engineering(pre7_alltrain, tf_idf=1, train=pre7_alltrain, word2vec=None, word_count=None)\n",
        "test_tf = feature_engineering(pre7_test, tf_idf=1, train=pre7_alltrain, word2vec=None, word_count=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV8EJD_JZmAO",
        "outputId": "72fac48b-3327-41c6-c774-16b1ded635df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.7558780493242784, 0.7643333333333333, 0.7792575111598599, 0.7643333333333333]]\n"
          ]
        }
      ],
      "source": [
        "lg = LogisticRegression(C=70,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_tf, y_train)\n",
        "y_pred = lg.predict(test_tf)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieODWwY6ZmAO",
        "outputId": "33534300-7f18-4bfc-d31a-5d6e4e64e2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.7558780493242784, 0.7643333333333333, 0.7792575111598599, 0.7643333333333333]]\n"
          ]
        }
      ],
      "source": [
        "lg = LogisticRegression(C=70,max_iter=1500,solver='newton-cg')\n",
        "lg.fit(train_tf, y_train)\n",
        "y_pred = lg.predict(test_tf)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU0Mhj8TBEBC",
        "outputId": "fe680b62-7096-417c-9430-39490e4b5182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.7558780493242784, 0.7643333333333333, 0.7792575111598599, 0.7643333333333333]]\n"
          ]
        }
      ],
      "source": [
        "lg = LogisticRegression(C=70,max_iter=2000,solver='newton-cg')\n",
        "lg.fit(train_tf, y_train)\n",
        "y_pred = lg.predict(test_tf)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DINcfaB9ZmAO"
      },
      "source": [
        "## 5.5 Analysis the result of TF-IDF + LIWC "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdspkHwvIogS"
      },
      "source": [
        "### 5.5.1 The original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG-aaFf5ZmAO"
      },
      "outputs": [],
      "source": [
        "train2 = train1.copy()\n",
        "test2 = test1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoW1ixq5ZmAO",
        "outputId": "33fd9b7c-f45a-41de-db1f-712ca7e4c3aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Cross Validation\n",
        "pre7_train = preprocess_text(train2, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
        "pre7_test = preprocess_text(test2, replace=1, remove_punctuation=1, lower=1,stopword=1,noisy=1,frequency_words=1,scared_word=1,lemmatization=1)\n",
        "y_train = pre7_train['Verdict']\n",
        "y_test = pre7_test['Verdict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adN4fYQnZmAP"
      },
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "train_tf = feature_engineering(pre7_train, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)\n",
        "test_tf = feature_engineering(pre7_test, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjBhcktIZmAP"
      },
      "outputs": [],
      "source": [
        "#LIWC\n",
        "LIWC_train = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/LIWC/df_train_spacy.csv\")\n",
        "LIWC_test = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/LIWC/df_test_spacy.csv\")\n",
        "\n",
        "train_y = LIWC_train['Verdict']\n",
        "test_y = LIWC_test['Verdict']\n",
        "LIWC_train1 = LIWC_train.copy()\n",
        "LIWC_test1 = LIWC_test.copy()\n",
        "LIWC_train1.drop(columns=['Verdict','Text'],inplace=True)\n",
        "LIWC_test1.drop(columns=['Verdict','Text'],inplace=True)\n",
        "train_X = LIWC_train1\n",
        "test_X = LIWC_test1\n",
        "#Normalization\n",
        "\n",
        "train_X_n = preprocessing.scale(train_X)\n",
        "test_X_n = preprocessing.scale(test_X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOSTc5ZNZmAP"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix,hstack\n",
        "train_X_n_m = csr_matrix(train_X_n)\n",
        "train_X_total = hstack([train_tf,train_X_n_m])\n",
        "test_X_n_m = csr_matrix(test_X_n)\n",
        "test_X_total = hstack([test_tf,test_X_n_m])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSdpqxUXZmAP",
        "outputId": "c86bbf66-7ceb-4b64-a078-82e4467041d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5438564372888904, 0.5466666666666666, 0.5425634864321488, 0.5466666666666666]]\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "lg = LogisticRegression(C=0.01,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_X_total, train_y)\n",
        "y_pred = lg.predict(test_X_total)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Aibw8e9Isoc"
      },
      "source": [
        "### 5.5.2 Small dataset (45854 news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qe6uoinRa8Y",
        "outputId": "6c50619a-4c58-4e30-87fd-d23388bb7d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "train3 = train2.sample(frac=1.0, random_state=10)\n",
        "train_tf2 = train3[0:45854]\n",
        "test_tf = train3[45854:48854]\n",
        "\n",
        "#Cross Validation\n",
        "pre7_train = preprocess_text(train_tf2,replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(test_tf, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "y_train = pre7_train['Verdict']\n",
        "y_test = pre7_test['Verdict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nztD_hXxSjH4"
      },
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "train_tf = feature_engineering(pre7_train, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)\n",
        "test_tf = feature_engineering(pre7_test, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BreYH64wZmAQ",
        "outputId": "edf807cd-7221-4d76-b676-11f0edd7431a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.8776810966324724, 0.8871854145767, 0.8843099723863017, 0.8735467669220807]]\n"
          ]
        }
      ],
      "source": [
        "#LIWC\n",
        "LIWC_all = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/LIWC/df_train_spacy.csv\")\n",
        "LIWC_all2 = LIWC_all.sample(frac=1.0, random_state=10)\n",
        "LIWC_train = LIWC_all2[0:45854]\n",
        "LIWC_test = LIWC_all2[45854:48854]\n",
        "LIWC_train1 = LIWC_train.copy()\n",
        "LIWC_test1 = LIWC_test.copy()\n",
        "LIWC_y = LIWC_train['Verdict']\n",
        "LIWC_y = LIWC_test['Verdict']\n",
        "LIWC_train1.drop(columns=['Verdict','Text'],inplace=True)\n",
        "LIWC_test1.drop(columns=['Verdict','Text'],inplace=True)\n",
        "train_X = LIWC_train1\n",
        "test_X = LIWC_test1\n",
        "\n",
        "#Normalization\n",
        "\n",
        "train_X_n = preprocessing.scale(train_X)\n",
        "test_X_n = preprocessing.scale(test_X)\n",
        "\n",
        "\n",
        "train_X_n_m = csr_matrix(train_X_n)\n",
        "train_X_total = hstack([train_tf,train_X_n_m])\n",
        "test_X_n_m = csr_matrix(test_X_n)\n",
        "test_X_total = hstack([test_tf,test_X_n_m])\n",
        "\n",
        "#The result in the training set\n",
        "lg = LogisticRegression(C=0.01,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_X_total, y_train)\n",
        "y_pred = lg.predict(train_X_total)\n",
        "score = []\n",
        "f1_macro = f1_score(y_train, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "precision_macro = precision_score(y_train, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_train, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCClKEPGKjrN",
        "outputId": "01390704-af36-480e-9fb6-302c90edb27d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.8518225399944388, 0.8476666666666667, 0.8650446208834282, 0.8544400201185647]]\n"
          ]
        }
      ],
      "source": [
        "lg = LogisticRegression(C=0.01,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_X_total, y_train)\n",
        "y_pred = lg.predict(test_X_total)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2tiuUoiK0id"
      },
      "source": [
        "## 5.6 Analysis of TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UknDYM2tLA8a",
        "outputId": "4a031263-0513-4c3d-b771-e56ce200bdda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[[0.9999734347469967, 0.9999781916517643, 0.9999850933158428, 0.9999617795444122]]\n"
          ]
        }
      ],
      "source": [
        "train4 = train2.sample(frac=1.0, random_state=10)\n",
        "X_train = train4[0:45854]\n",
        "X_test = train4[45854:48854]\n",
        "pre7_train = preprocess_text(X_train, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(X_test, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "y_train = X_train['Verdict']\n",
        "y_test = X_test['Verdict']\n",
        "\n",
        "\n",
        "# TF-IDF\n",
        "train_tf = feature_engineering(pre7_train, tf_idf=1, train=X_train, word2vec=None, word_count=None)\n",
        "test_tf = feature_engineering(pre7_test, tf_idf=1, train=X_train, word2vec=None, word_count=None)\n",
        "\n",
        "#The result in the training set\n",
        "lg = LogisticRegression(C=70,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_tf, y_train)\n",
        "y_pred = lg.predict(train_tf)\n",
        "score = []\n",
        "f1_macro = f1_score(y_train, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "precision_macro = precision_score(y_train, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_train, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUBNOMOMY9DQ",
        "outputId": "880e6702-db8f-4248-efa5-82fbef04e231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.9696553197571132, 0.9703333333333334, 0.9719263584310721, 0.9674837630168126]]\n"
          ]
        }
      ],
      "source": [
        "#The result in the test set\n",
        "lg = LogisticRegression(C=70,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_tf, y_train)\n",
        "y_pred = lg.predict(test_tf)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GbvGT0grZmAR"
      },
      "source": [
        "## 5.7 Larger and Smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f37oXM4gNJy5"
      },
      "outputs": [],
      "source": [
        "train5 = train1.sample(n=20000, random_state=1)\n",
        "test5 = test1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZevtteTZmAR"
      },
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "train_tf_all_s = feature_engineering(pre7_train, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)\n",
        "test_tf_all_s = feature_engineering(pre7_test, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3ytPGdoZmAR"
      },
      "outputs": [],
      "source": [
        "train_y_s = pre7_train['Verdict']\n",
        "y_test_s = pre7_test['Verdict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ubX7K_mZmAS",
        "outputId": "80b8f06a-538e-4a7b-ed42-2bb594e5b97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.7469248824124232, 0.7556666666666667, 0.7691425162884407, 0.7556666666666667]]\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "lg = LogisticRegression(C=40,max_iter=1000,solver='newton-cg')\n",
        "lg.fit(train_tf_all_s, train_y_s)\n",
        "y_pred = lg.predict(test_tf_all_s)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test_s, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test_s, y_pred)\n",
        "precision_macro = precision_score(y_test_s, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test_s, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Logistic Regression",
      "provenance": [],
      "collapsed_sections": [
        "5S7DYEfkZl_-",
        "Qhb7SSV1ZmAD",
        "zbQ7wMzuZI-9",
        "-m5phJ4lZmAH",
        "I-YoYOcMZmAI",
        "EIfQcbh6ZmAJ",
        "ovQcMHyPqbIE",
        "-puavU86qei-",
        "pHBpN3yeqjE2",
        "9_cJbjCkjIHt",
        "pFpUrZF6jNez",
        "FyNZXBDBZmAM",
        "f3tgfzaKZmAN",
        "RVpAkyAOZmAN",
        "gBqziuEJ4nps",
        "DINcfaB9ZmAO",
        "bdspkHwvIogS",
        "-Aibw8e9Isoc",
        "h2tiuUoiK0id",
        "GbvGT0grZmAR"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}