{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import necessary libraries"
      ],
      "metadata": {
        "id": "ChIrGJP-Xbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import scipy\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from textblob import Word\n",
        "from sklearn.utils import resample\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix,hstack\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zvmNWzHUXckp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0DyHQ-VXjGN",
        "outputId": "0e50c29e-7d8d-48e1-cfd9-2bfc48851baf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Read Data"
      ],
      "metadata": {
        "id": "l8saQo-gXkgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec3_EcrJXlsC",
        "outputId": "99252600-29af-4085-83e2-b1d3c0111bb9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read files\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/raw_data/fulltrain.csv\",header=None)\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/CS4248_Project/raw_data/balancedtest.csv\",header=None)"
      ],
      "metadata": {
        "id": "2xknxQOOXnRn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns = ['Verdict','Text']\n",
        "test.columns = ['Verdict','Text']"
      ],
      "metadata": {
        "id": "6rh0mX7VXqJl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = train.copy()"
      ],
      "metadata": {
        "id": "RR8LsMomX39B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocessing for Text"
      ],
      "metadata": {
        "id": "Bd3xQUiTXvGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(s, replace=None, remove_punctuation = None, lower=None,stopword=None,frequency_words=None,scared_word=None, noisy=None, stemming=None,lemmatization=None):\n",
        "    #Throw an error is both stemming and lemmatization are not None\n",
        "\n",
        "    s1 = s.copy()\n",
        "    if stemming is not None and lemmatization is not None:\n",
        "        raise ValueError('Stemming and Lemmatization cannot both be not None!')\n",
        "\n",
        "\n",
        "    if replace is not None:\n",
        "        #Replace URLs with 'webaddress'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
        "                                  'webaddress',regex=True)\n",
        "        #Replace email address with 'email'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
        "                                 'emailaddress',regex=True)\n",
        "        #Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
        "        s1['Text'] = s1['Text'].str.replace(r'£|\\$', 'moneysymb',regex=True)\n",
        "\n",
        "        #Replace percentage symbols with 'percentage'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'%', 'percentage',regex=True)\n",
        "\n",
        "        #Replace 10 digit phone number\n",
        "        s1['Text'] = s1['Text'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
        "                                  'phonenumbr',regex=True)\n",
        "        # Replace numbers with 'numbr'\n",
        "        s1['Text'] = s1['Text'].str.replace(r'\\d+(\\.\\d+)?', 'numbr',regex=True)\n",
        "    #Remove punctuation\n",
        "    if remove_punctuation is not None:\n",
        "        s1['Text'] = s1['Text'].apply(lambda x: re.sub(r'[^\\w\\s\\d]', '', x))\n",
        "\n",
        "    #Transform to lower letter\n",
        "    if lower is not None:\n",
        "        s1['Text'] = s1['Text'].apply(lambda x: x.lower())\n",
        "\n",
        "    #Remove the stopwords\n",
        "    if stopword is not None:\n",
        "        stop=stopwords.words('english')\n",
        "        s1['Text']=s1['Text'].apply(lambda sen:\" \".join(x for x in sen.split() if x not in stop))\n",
        "\n",
        "    #Remove the frequency words\n",
        "    if frequency_words is not None:\n",
        "        freq=pd.Series(' '.join(s).split()).value_counts()[:10]\n",
        "        freq=list(freq.index)\n",
        "        s1['Text']=s1['Text'].apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))\n",
        "\n",
        "    # Remove the scarce word\n",
        "    if scared_word is not None:\n",
        "        scared = pd.Series(' '.join(s).split()).value_counts()[-10:]\n",
        "        scared = list(scared.index)\n",
        "        s1['Text'] = s1['Text'].apply(lambda sen: \" \".join(x for x in sen.split() if x not in scared))\n",
        "\n",
        "    #Noisy Removal\n",
        "    if noisy is not None:\n",
        "        #remove non-ascii\n",
        "        s1['Text']= s1['Text'].apply(lambda x: re.sub(\"(\\\\W)\",\" \",x))\n",
        "        #remove whitespace\n",
        "        s1['Text']=s1['Text'].apply(lambda x: x.strip())\n",
        "\n",
        "    #Stemming\n",
        "    if stemming is not None:\n",
        "        ps = PorterStemmer()\n",
        "        s1['Text']=s1['Text'].apply(lambda x:\" \".join(ps.stem(word) for word in x.split()))\n",
        "\n",
        "    #Lemmatization\n",
        "    if lemmatization is not None:\n",
        "        nltk.download('wordnet')\n",
        "        s1['Text']= s1['Text'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "    return s1"
      ],
      "metadata": {
        "id": "LcDDPWwRXwiZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering"
      ],
      "metadata": {
        "id": "aY8VtYTCXxw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine three feature engineering methods into one class\n",
        "def feature_engineering(s, train=None,tf_idf=None, word2vec=None, word_count=None):\n",
        "    #1. TF-IDF\n",
        "    s1 = s.copy()\n",
        "    if tf_idf is not None:\n",
        "        tfv = TfidfVectorizer(min_df=3,  max_features=None,strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n",
        "        # Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
        "        tfv.fit(list(train['Text']))\n",
        "        features =  tfv.transform(s1['Text'])\n",
        "    #2. Word2Vec\n",
        "    if word2vec is not None:\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        features = []\n",
        "        for sentence in s1['Text']:\n",
        "            doc = nlp(sentence)\n",
        "            features.append(doc.vector)\n",
        "    #3. Word-count document\n",
        "    if word_count is not None:\n",
        "        #Instantiate the vectorizer\n",
        "        count_vectorizer = CountVectorizer()\n",
        "        features = count_vectorizer.fit_transform(s1['Text'])\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "LN-UwtOXXyua"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. XGBoost"
      ],
      "metadata": {
        "id": "koMFV3rSX0Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre7_train = preprocess_text(train1, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(test, replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "# # TF-IDF\n",
        "train_tf = feature_engineering(pre7_train, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)\n",
        "test_tf = feature_engineering(pre7_test, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp4Nbn1PX2Es",
        "outputId": "d7b8e832-9259-4672-ea63-1750443de03c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = pre7_train['Verdict']\n",
        "y_test = pre7_test['Verdict']"
      ],
      "metadata": {
        "id": "aCcOOau0X6GB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly choose 10% dataset from the original one\n",
        "X_train, X_one_ten, y_train, y_one_ten = train_test_split(train_tf, train_y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "NL_oA1fRX67h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "xgb.fit(X_one_ten, y_one_ten)\n",
        "y_pred = xgb.predict(test_tf)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0oA8KnKX8dR",
        "outputId": "6d93388a-1569-4356-d850-6a410675baf7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5468871920880969, 0.561, 0.5709124197031853, 0.5609999999999999]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Hyperparameter Tunning"
      ],
      "metadata": {
        "id": "pIEJa8u4X9Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Learning Rate\n",
        "lr = [0.001,1,5,10,20]\n",
        "result = []\n",
        "for i in lr:\n",
        "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=i, max_depth=1, random_state=0)\n",
        "    clf.fit(X_one_ten, y_one_ten)\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVgJzW16X_Ro",
        "outputId": "ef1d733e-9fa8-46fe-f0bb-8cc98f11f1b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.5468871920880969, 0.23466887744501796, 0.1128348906971933, 0.16604359824495227]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Learning Rate\n",
        "n_estimators = [200,500,100]\n",
        "result = []\n",
        "for i in n_estimators:\n",
        "    clf = GradientBoostingClassifier(n_estimators=i, learning_rate=1, max_depth=1, random_state=0)\n",
        "    clf.fit(X_one_ten, y_one_ten)\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_ruBPbIYAkP",
        "outputId": "ac384a4e-252d-4918-9f03-68f61ff3eb2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.575792082245718, 0.5947006450429947, 0.5468871920880969]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Max Depth\n",
        "max_depth = [3,5]\n",
        "result = []\n",
        "for i in max_depth:\n",
        "    clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=1, max_depth=i, random_state=0)\n",
        "    clf.fit(X_one_ten, y_one_ten)\n",
        "    y_pred = clf.predict(test_tf)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    result.append(f1_macro)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt38CamnYCWo",
        "outputId": "afb99a79-2787-4b32-e967-461ec1201017"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6056158475293896, 0.609257245227721]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Best Model"
      ],
      "metadata": {
        "id": "a9T-HtfsYC03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=1, max_depth=5, random_state=0)\n",
        "clf.fit(train_tf, train_y)\n",
        "y_pred = clf.predict(test_tf)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "result.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "S4pMUNomYGQu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Large and Small Dataset"
      ],
      "metadata": {
        "id": "WDJnsio0YGxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train5 = train1.sample(n=20000, random_state=1)\n",
        "test5 = test"
      ],
      "metadata": {
        "id": "Y8Bz-GwrYK09"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre7_train = preprocess_text(train5,replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)\n",
        "pre7_test = preprocess_text(test5,replace=1, remove_punctuation=1, lower=1,stopword=None,noisy=None,frequency_words=None,scared_word=None,lemmatization=1)"
      ],
      "metadata": {
        "id": "glNG0p0LYLs6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "train_tf_all_s = feature_engineering(pre7_train, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)\n",
        "test_tf_all_s = feature_engineering(pre7_test, tf_idf=1, train=pre7_train, word2vec=None, word_count=None)"
      ],
      "metadata": {
        "id": "8MmGfm6sYM0J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y_s = pre7_train['Verdict']\n",
        "y_test_s = pre7_test['Verdict']"
      ],
      "metadata": {
        "id": "XX6OYtIiYNp9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lg = GradientBoostingClassifier(n_estimators=500, learning_rate=1, max_depth=i, random_state=0)\n",
        "lg.fit(train_tf_all_s, train_y_s)\n",
        "y_pred = lg.predict(test_tf_all_s)\n",
        "score = []\n",
        "f1_macro = f1_score(y_test_s, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test_s, y_pred)\n",
        "precision_macro = precision_score(y_test_s, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test_s, y_pred, average='macro')\n",
        "score.append([f1_macro,accuracy,precision_macro,recall_macro])\n",
        "print(score)"
      ],
      "metadata": {
        "id": "VC3QWrXbYOZj"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}